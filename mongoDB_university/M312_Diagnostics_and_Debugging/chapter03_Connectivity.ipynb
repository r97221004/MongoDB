{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75c0183d",
   "metadata": {},
   "source": [
    "# What Connectivity Issues Look Like in an Application"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c5eea0f3",
   "metadata": {},
   "source": [
    "So your application is chugging along(穩穩進行), and then some connectivity issues arise.\n",
    "\n",
    "What do you see in your application that tells you that your problem is an issue with connectivity?\n",
    "\n",
    "So most likely what you're going to see are timeouts and connection errors.\n",
    "\n",
    "Some might be detected on the client side, others on the server side.\n",
    "\n",
    "And it's also possible that you'll see long hangs that potentially don't result in timeouts depending on how your application is set up.\n",
    "\n",
    "Your application should be smart about these issues.\n",
    "\n",
    "It should be able to turn a long hang into an error that it catches, and it should log a message somewhere describing the type of issue.\n",
    "\n",
    "Finally, it should also generate a notification for you, the DBA.\n",
    "\n",
    "But let's be realistic.\n",
    "\n",
    "We live in an imperfect world, developers work under constraints, and things aren't always set up to be the way we'd like them to be.\n",
    "\n",
    "So we may need to do some digging to find out the root cause.\n",
    "\n",
    "What might it be?\n",
    "\n",
    "Well, it could be any of the following-- incorrect URIs, interference from firewalls, or just things out of your control.\n",
    "\n",
    "Why might you have an incorrect URI?\n",
    "\n",
    "Well, maybe some of your servers have valid URIs for one another, which don't resolve from the point of view of the application.\n",
    "\n",
    "The servers can all see each other and everything works until your primary goes down, and your application loses contact with the entire replica set.\n",
    "\n",
    "Alternatively, maybe your servers have non-static IP addresses, so restarts cause you to lose servers unless you intervene.\n",
    "\n",
    "Besides issues with URIs, you might also have a problem with a firewall.\n",
    "\n",
    "Usually, the most basic firewall issues are caught early, 'cause they often prevent your system from getting set up until they're fixed.\n",
    "\n",
    "But sometimes you might have a situation where your IP whitelist only permits the application to see some of the database servers and not others.\n",
    "\n",
    "Again, the issue arises when the wrong server goes down.\n",
    "\n",
    "Or maybe it's something out of your control.\n",
    "\n",
    "Maybe a DDoS is hammering your servers, or your ISP isn't doing its job and packets just aren't getting through.\n",
    "\n",
    "We'll take a closer look at these issues and similar ones in this chapter.\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24dee87",
   "metadata": {},
   "source": [
    "# Timeouts"
   ]
  },
  {
   "cell_type": "raw",
   "id": "32566324",
   "metadata": {},
   "source": [
    "Timeouts(超時) are one of those things that sometimes are just neglected, either because we believe that they are mythical creatures that don't exist, or because we don't think they are very important.\n",
    "\n",
    "But I can assure you, that they are very, very real, and very important.\n",
    "\n",
    "Timeouts are not a bad thing, per se.\n",
    "\n",
    "They are actually a guarantee that a given operation does not take forever."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a245ac14",
   "metadata": {},
   "source": [
    "<img src=\"img/136.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "469e09b4",
   "metadata": {},
   "source": [
    "Either the communication between one node and another for the network, or an operation going on, or even just a process calling a task, like an IO task or a network task, or just calling some other process to do something that it needs.\n",
    "\n",
    "In either case, timeouts are not the problem, they are guarantees.\n",
    "\n",
    "However, when they do happen, that means that something in our system is not responding, or taking longer than expected, or longer than acceptable.\n",
    "\n",
    "In a MongoDB cluster, there are a few timeouts that can occur, and they are reported in the form of timeout errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5a6e2b",
   "metadata": {},
   "source": [
    "<img src=\"img/137.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "afde442d",
   "metadata": {},
   "source": [
    "We're going to have Server Selection Timeout, which can occur on our client side, where no MongoDB server is available after connection.\n",
    "\n",
    "So if we have an established connection to a MongoDB, and we write, or do an insert operation, but in the meantime that node became idle or not present, or the network was simply gone, we will get back a MongoDB Server Selection Timeout error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71814d50",
   "metadata": {},
   "source": [
    "<img src=\"img/138.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b17f57a2",
   "metadata": {},
   "source": [
    "WTimeout is a flag of our write concern that applications can set to acknowledge writes.\n",
    "\n",
    "We can use it to specify that, given an insert or write operation, if we require that operation to be confirmed by all nodes involved in the W node flag, if they do not comply within the specified wTimeout, a wTimeout error is sent back within the response saying well, you know what, we tried, but we could not comply with the timeout for that write acknowledge across, in this case, all three nodes within two milliseconds.\n",
    "\n",
    "Now that does not necessarily mean that our write operation did not complete it.\n",
    "\n",
    "We will see about the details in a few seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8931f831",
   "metadata": {},
   "source": [
    "<img src=\"img/139.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e7c240ac",
   "metadata": {},
   "source": [
    "Another time out that we can set from the application side is max timeout in milliseconds parameter for a given cursor operation.\n",
    "\n",
    "While wTimeout is related with write concern, max time milliseconds is a cursor operations parameter, which means that if we extend a given operation more than the specified max time ms, we are going to get back an execution timeout.\n",
    "\n",
    "Our operation could not complete on the expected maximum time for it to be accomplished.\n",
    "\n",
    "So if we have a connection, percent in operation, and if it doesn't complete by then, we will get a timeout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c7e22d",
   "metadata": {},
   "source": [
    "<img src=\"img/140.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "30e7c148",
   "metadata": {},
   "source": [
    "And finally, we have a network timeout, which occurs while a connection, or even an instruction, takes longer than the admissible expected time to complete at network level.\n",
    "\n",
    "It can be related with socket timeouts, networks, impairments, some wobbly switches not deciding in due time what to connect and when to connect to.\n",
    "\n",
    "Those are generally out of control of MongoDB, they are mostly related with network configuration itself.\n",
    "\n",
    "But now let's see some of these, especially wTimeout and execution timeout in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20f5134",
   "metadata": {},
   "source": [
    "# Timeouts- 有關 write concern 的實作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f224bbb",
   "metadata": {},
   "source": [
    "<img src=\"img/141.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "21e429a3",
   "metadata": {},
   "source": [
    "I'm going to connect to my Vagrant box, my m312 vagrant environment, and I'm going to launch a replica set that I'm going to call timeouts with three nodes and where their data sets will going to be specified on this folder that I just created as well, called timeouts and starting on port 27000.\n",
    "\n",
    "Once my replica set is initiated, I'll connect to one of the nodes and express the following command.\n",
    "\n",
    "Here, I'm going to insert this document, Hello World, with a write concern expecting three nodes to acknowledge this write within one millisecond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87646a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first time connecting\n",
    "db.foo.insertOne( { hello: 'world' }, { writeConcern: { w: 3, wtimeout:1 } } )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "65974765",
   "metadata": {},
   "source": [
    "So anything that takes more than one millisecond, I will come back with an error saying hey, I could not confirm that three nodes of your replica set were able to acknowledge the write within one millisecond in total.\n",
    "\n",
    "So if I do so, and since I'm very bullish about this expected timeout, I actually get back a write concern failed error saying well you know what we tried but we were not able to get the confirmation within one millisecond that all three nodes actually got that write operation.\n",
    "\n",
    "Now this happens because we just launched our replica set.\n",
    "\n",
    "Everything is in the same box but once we launch and write something to that newly boot up MongoD, it will need to do a couple of things like writing into an index or creating index files into the collection files.\n",
    "\n",
    "So those probably are going to take more than one millisecond alone.\n",
    "\n",
    "Therefore, I can not guarantee that that write was acknowledged by three nodes within one millisecond.\n",
    "\n",
    "That would be pretty awesome, wouldn't it?\n",
    "\n",
    "If I try the exact same instruction, new document with the same set of parameters, but after all the files have been initialized and all, I will actually be able to insert it.\n",
    "\n",
    "Once everything is initialized and I have everything up and running, actually, the nodes are able to communicate back that they acknowledged the writes below one millisecond, which is pretty awesome.\n",
    "\n",
    "The important thing here is that we need to make sure we understand what's going on under the covers.\n",
    "\n",
    "As I can see here, I attempted three inserts and got all of them, is that although I was not able to confirm that all three nodes were able to acknowledge below one millisecond, the write is still acknowledged in the primary.\n",
    "\n",
    "We will always notify you back to the application saying what you expected was not able to be met.\n",
    "\n",
    "But, that doesn't mean that the write will be rolled back or will not succeed on the primary nodes or even the secondary nodes, it just means it doesn't succeed within the time I'm expecting the system to comply with.\n",
    "\n",
    "So, if by anytime your developers start seeing messages like this, make sure they realize what is going on and be assuring them that the data is safely stored in the primaries at least, just that their expectations are not able to be met.\n",
    "\n",
    "And let's not get confused by some other aspect of this write concern which is not related with the w timeout itself.\n",
    "\n",
    "Let's go ahead and connect to one of the secondary nodes that is listening on port 27001 and let's shut down this server.\n",
    "\n",
    "And let me connect back to my test database where I was inserting my data.\n",
    "\n",
    "In this case, the message will be the same but even if we increase the w timeout, just let's say 100 milliseconds, we will still get write concern failed within the replication timeout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830a89e9",
   "metadata": {},
   "source": [
    "<img src=\"img/142.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a03bb05c",
   "metadata": {},
   "source": [
    "This is only due to the fact that if one of our nodes is completely down, it is impossible, regardless of the number of w timeout value that we set, to comply back because until it gets up and running probably, it will be more time than 100 milliseconds.\n",
    "\n",
    "A slight variation will be to request a number of nodes that pure and simply isn't there.\n",
    "\n",
    "In this case, we can not satisfy concern will be sent back because there's not enough data bearing nodes to actually comply with our request.\n",
    "\n",
    "Our data will still be stored in the primary and all living secondaries but regardless of the w timeout.\n",
    "\n",
    "We are expecting more nodes than what our current configuration of the replica set has in terms of data bearing nodes.\n",
    "\n",
    "So this will not be satisfied not because of the w timeout, but because we are requesting something that's purely simply not possible.\n",
    "\n",
    "These are the kind of errors that your developers might see in a production environment.\n",
    "\n",
    "Make sure they understand clearly what those messages refer of and how they can deal with those from the application side.\n",
    "\n",
    "Now that we've seen what w timeout is, let's look into another nice timeout error that can be controlled and set by the application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9576625f",
   "metadata": {},
   "source": [
    "# Timeouts- 有關 $maxTimeMs 的實作"
   ]
  },
  {
   "cell_type": "raw",
   "id": "95090ef6",
   "metadata": {},
   "source": [
    "Our max time milliseconds value.\n",
    "\n",
    "For this, I'm going to be using the help of my friends.\n",
    "\n",
    "I'm going to import them into our data sets, test, and a collection called friends.\n",
    "\n",
    "Once this is fully imported, I'm going to be able to do some queries using my maxTimeMS parameter.\n",
    "\n",
    "I'm going to connect to my primary.\n",
    "\n",
    "And here, I'm going to express the following query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d36250",
   "metadata": {},
   "source": [
    "<img src=\"img/143.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b8f88cfe",
   "metadata": {},
   "source": [
    "I'm going to find from my friends everyone that has an e, h value within their email.\n",
    "\n",
    "But I'm going to set the special parameter called maxTimeMS and I'm going to say, anything above one millisecond, if it takes more to complete, I don't want to know about it.\n",
    "\n",
    "I want it to be killed.\n",
    "\n",
    "And I don't want to have anything to do with that.\n",
    "\n",
    "So I'm going to throw that into a cursor.\n",
    "\n",
    "And I'm going to iterate over the full amount of the results of my cursor.\n",
    "\n",
    "Boom, I get an operation exceeded time, exceeded time limit code.\n",
    "\n",
    "And this will result in an execution timeout error.\n",
    "\n",
    "I can still see some of the results.\n",
    "\n",
    "And this is because the cursor will act in batches.\n",
    "\n",
    "So the first batch will be very, very quick because data will be in memory.\n",
    "\n",
    "But then after that, a few pages will need to be fetched from disk or some new indexes will need to be traversed to get the result back.\n",
    "\n",
    "And some of those operations will take more than the expected one millisecond maxTimeMS.\n",
    "\n",
    "And therefore I will get that error back.\n",
    "\n",
    "From the application, we will need to manage these timeouts.\n",
    "\n",
    "And they are a signal that we may want to change our settings, maybe having less aggressive timeouts for maxTimeMS or do something about our query that will not get into trouble more than our SLA, for example, will require.\n",
    "\n",
    "So a few things that we've learned today, timeouts generate some exceptions.\n",
    "\n",
    "We've seen that.\n",
    "\n",
    "Your application needs to be ready to catch those timeouts.\n",
    "\n",
    "So if your developers come up to you and say that they are seeing those kinds of errors and asking why, you should explain what those mean for their application and how they should be treating them.\n",
    "\n",
    "And timeouts should be set so they happen rarely, so that your application can respond properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3930c1a0",
   "metadata": {},
   "source": [
    "### 範例01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4496a1",
   "metadata": {},
   "source": [
    "wtimeout can be set by the application to guarantee that:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d4ccc170",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "# If a write is acknowledged, the replica set has applied the write on a number of servers within the specified wtimeout.\n",
    "  \n",
    "Incorrect answers are:\n",
    "\n",
    "# MongoDB will roll back the write operations if the nodes do not acknowledge the write operation within the specified\n",
    "  wtimeout.\n",
    "MongoDB does not roll back writes that fail to achieve the write concern's \"w\" parameter within the specified wtimeout. If the primary has received them, they are most likely on the primary, and possibly other servers. Any further writes will need to be done by the application.\n",
    "\n",
    "# The application will learn how many servers have performed the write within wtimeout.\n",
    "All the application knows is that it has not received acknowledgment within wtimeout. No further information is known at this time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff1f4b9",
   "metadata": {},
   "source": [
    "# Closing and Dropping Connections"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee95be49",
   "metadata": {},
   "source": [
    "In this lesson, we'll be talking about connection management, especially around closing and dropping connections.\n",
    "\n",
    "First we will be talking about greed.\n",
    "\n",
    "No, not Wall Street kind of greed, that's a matter of discussion for another type of class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ced913",
   "metadata": {},
   "source": [
    "<img src=\"img/144.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "295d4ffb",
   "metadata": {},
   "source": [
    "Today, what we are going to be focusing on is discussing how resource greedy applications and keeping connections for extended periods of time while leaving them idle, most importantly, what resource allocation, especially memory, will be looking like.\n",
    "\n",
    "We will also be looking into how to detect poorly connect configured hosts regarding connectivity, related pedometers.\n",
    "\n",
    "So we're going to be looking into how these can affect your deployment.\n",
    "\n",
    "And finally, we are going to be looking into what happens when the new primaries election within the replica set.\n",
    "\n",
    "This is not necessarily a problem.\n",
    "\n",
    "But once an election takes place, what happens to your application from the connection perspective and how to then handle the subsequent connection operations that will be affected by that election.\n",
    "\n",
    "Let's consider greedy connections that your applications might have or might be configured and how to see their effects.\n",
    "\n",
    "In the hand outs of this lesson, you will find a script called, make lots of connections to servers.py.\n",
    "\n",
    "If you want to follow up with this lesson, you should download this into your vagrant environment shared folder.\n",
    "\n",
    "So in my case, I have it here for my m312 vagrant environment.\n",
    "\n",
    "Now what this script will do is simulate one of the worst scenarios possible.\n",
    "\n",
    "One that creates several different connections from an application to a database that end up not doing much.\n",
    "\n",
    "And actually in this case, we are just creating the connection and that's it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1598efeb",
   "metadata": {},
   "source": [
    "<img src=\"img/145.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cdc39c36",
   "metadata": {},
   "source": [
    "We are not even reading anything from this particular application.\n",
    "\n",
    "We're just going to throw a bunch of threads or processes, all of them will create a bunch of different connections.\n",
    "\n",
    "And they will idle for a long period of time.\n",
    "\n",
    "Now first to understand what's going on, we will need to use a couple of tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3109a912",
   "metadata": {},
   "source": [
    "<img src=\"img/146.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9042405e",
   "metadata": {},
   "source": [
    "We're going to be using Mongostats, that as you know allows us to see what's going on from a real time perspective on our server.\n",
    "\n",
    "We'll definitely be using serverStatus to get some information of a couple of parameters that we're going to be looking into.\n",
    "\n",
    "From the OS perspective, we're going to be using free.\n",
    "\n",
    "And obviously, you're going to be tailing and catting and seeing what the log file has to offer in terms of information of what's going on when we have such a setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4c526c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5fb1f085",
   "metadata": {},
   "source": [
    "Apart from the make lots of connections to servers file, in the handouts of this lesson, you're also going to see this CFG file, connections_singlenode file.\n",
    "\n",
    "Make sure you also place that into shared, so we can use it from our m312-vagrant-env box.\n",
    "\n",
    "Once you have it, the next step to do is basically connect to our bigger machine.\n",
    "\n",
    "Once we are in our vagrant box, let's just spin up a mongod, passing on as parameter, then -f and the configuration file that we recently downloaded from the lessons handout.\n",
    "\n",
    "Now, what this script does is sets up a mongod that listens on port 27000-- 27,000.\n",
    "\n",
    "Now, it's time to connect our super inefficient file, our make lots of connections to servers.py, which, in turn, takes a few parameters to operate.\n",
    "\n",
    "As you can see here, we need to specify a host, the ports, and if we were using a replica set name, and then a number of connections.\n",
    "\n",
    "OK, we can do that.\n",
    "\n",
    "But before we do that, let's open another tab, and within our mongo m312 vagrant box, let's start our mongostat to start capturing some traffic on this box.\n",
    "\n",
    "So we know that our mongod is listening on port 27000.\n",
    "\n",
    "And we know that mongostat can do that.\n",
    "\n",
    "Now, this is pretty, pretty noisy over here, so there's a lot of different statistics being collected that I'm probably not that interested in.\n",
    "\n",
    "Let's filter out some of that by just outputting a few, like the number of commands, the number of dirty memory, used memory, virtual size, residence size, connections, and the time stamp.\n",
    "\n",
    "Once we do that, we are going to get a much, much cleaner output of our mongostat.\n",
    "\n",
    "OK, let me jump back to our main tab where I can go ahead and run my script, my make_lots_of_connections script.\n",
    "\n",
    "We're going to connect 27000, and we're going to launch 100 different connections.\n",
    "\n",
    "Actually, we're going to be launching 100 different processes of this make lots of connections to servers script.\n",
    "\n",
    "Once I run this, and if I go back to our mongo stats, we will see that our dirty and used memory are pretty low, around 0%.\n",
    "\n",
    "This is because we are not really allocating any data into our cache.\n",
    "\n",
    "We're basically just connecting to the instance.\n",
    "\n",
    "However, on the virtual size and the residence size we do see a bit of a bump.\n",
    "\n",
    "Once this script finishes, and I can see here is a drop between 102 connections to 2 connections, there's been a slight drop on the virtual size.\n",
    "\n",
    "The residence size, however, continues to be allocated.\n",
    "\n",
    "We're going to see about that in just a few moments.\n",
    "\n",
    "So we've seen that there's a bump on virtual size and residence size.\n",
    "\n",
    "Obviously, as well in the number of connections.\n",
    "\n",
    "So let's play around with this a little bit more.\n",
    "\n",
    "Let's go ahead, instead of connecting to 100 different processes, let's put 200 in place.\n",
    "\n",
    "Once I do that, I can see that now I have a significant bump again on my virtual size.\n",
    "\n",
    "So connections do occupy space in memory, especially in virtual size, but also in my residence size.\n",
    "\n",
    "So there is, in fact, an increase in the amount of memory that I need to allocate some incoming connections of my system.\n",
    "\n",
    "So let's increase it once more, the number of processes and connections that I'm going to do to my server.\n",
    "\n",
    "Let's bring it up to 600.\n",
    "\n",
    "And here is something a little bit odd that is happening.\n",
    "\n",
    "Although I have requested for 600, the number of connections that I see established to my mongod does not increase more than 200.\n",
    "\n",
    "Why is that?\n",
    "\n",
    "What is going on?\n",
    "\n",
    "Why such a low value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf3958e",
   "metadata": {},
   "source": [
    "<img src=\"img/147.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a6763dc2",
   "metadata": {},
   "source": [
    "By default, MongoDB allows incoming connections up to 65,536.\n",
    "\n",
    "This is the maximum amount of connections that a single node can establish.\n",
    "\n",
    "But 200 is far from this value.\n",
    "\n",
    "So what is going on?\n",
    "\n",
    "Why my connections value is only 200?\n",
    "\n",
    "Let's get out of my mongostat, and let's tail my log file.\n",
    "\n",
    "OK?\n",
    "\n",
    "I see that a lot of connections ended.\n",
    "\n",
    "Let's bring back our process to life, again, to see and capture what's going on.\n",
    "\n",
    "All right, now my logs are telling me a couple of important messages here.\n",
    "\n",
    "First of all, I'm starting new threads, and I'm getting connections being refused.\n",
    "\n",
    "This comes with a nice message saying that I'm refusing connections because I may not open more than 200.\n",
    "\n",
    "Now, we know that the default allows us to go to 65,000-plus connections.\n",
    "\n",
    "We probably can't afford to do that on this virtual machine.\n",
    "\n",
    "That's fine.\n",
    "\n",
    "But 200 is still a very low value.\n",
    "\n",
    "So what can be the root cause of this issue?\n",
    "\n",
    "And this brings us to a particular set that I would like to raise up-- how to detect poorly configured hosts.\n",
    "\n",
    "The first place to look for would not be our log file.\n",
    "\n",
    "It will probably be our configuration file.\n",
    "\n",
    "So let's have a look to that /shared/connections singlenode.cfg file that we are currently using in our system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe62e03",
   "metadata": {},
   "source": [
    "<img src=\"img/148.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e7e5580e",
   "metadata": {},
   "source": [
    "Now, as you can see here, we are artificially capping the maximum amount of incoming connections to 200.\n",
    "\n",
    "So even if we have an application trying to establish more than 200 connections, those plus 200 will be refused, which is a good thing, because we are specifically saying to the system, hey, do not allow more than 200 connections.\n",
    "\n",
    "So you might consider that what's limiting our ability in this particular case of establishing more connections is our own configuration.\n",
    "\n",
    "So be aware of this value if you're trying to debug why your application is not enabled to provide more connections than the ones that you are seeing through the mongostat.\n",
    "\n",
    "So if we want to get rid of that, let's just remove the max incoming connections to 200 from the configuration file.\n",
    "\n",
    "Let's go into our mongod, and let's shut down the server in a clean fashion.\n",
    "\n",
    "And let's bring our along with you back again with the new configuration place.\n",
    "\n",
    "Sweet, we now have removed that limitation of 200 from our configuration file.\n",
    "\n",
    "Let's bring our mongostat command back again so we can see that now we have our mongod up and running again.\n",
    "\n",
    "And let's try again to launch 600 connections from this script.\n",
    "\n",
    "Great!\n",
    "\n",
    "We can see that our mongod has 600 new connections established.\n",
    "\n",
    "And we see a bump, as well, in the virtual size memory allocation.\n",
    "\n",
    "Now, my question to you guys-- is that a good thing?\n",
    "\n",
    "Should we have close to a gigabyte of memory allocated for 600 connections that literally are not doing much?\n",
    "\n",
    "They're just been established.\n",
    "\n",
    "We're going to talk about that on the following lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f696636",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "id": "75c533e9",
   "metadata": {},
   "source": [
    "OK.\n",
    "\n",
    "So we see that there's a correlation between number of connections and virtual size being allocated.\n",
    "\n",
    "That's fine.\n",
    "\n",
    "So let's use a system tool, in our case, we're going to be using free command which gives us the indication of how is our memory being used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad38ddc2",
   "metadata": {},
   "source": [
    "<img src=\"img/149.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f4d60e28",
   "metadata": {},
   "source": [
    "A particular command that I really like to use in conjunction with free is watch.\n",
    "\n",
    "In this case, I'm going to watch this output every two seconds.\n",
    "\n",
    "So I can see here, what's my current utilization every two seconds of my memory.\n",
    "\n",
    "Which is great.\n",
    "\n",
    "Right now, I have nearly four gigabytes in total, I'm using 1.2, and I have 2.73.\n",
    "\n",
    "Awesome!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32809b7",
   "metadata": {},
   "source": [
    "<img src=\"img/150.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "157a345c",
   "metadata": {},
   "source": [
    "Once I run again my system with the 600 different threads in it, and if I watch my free again, I can see that the values increase rapidly and they stick with that increase for a while.\n",
    "\n",
    "So now instead of a few 1.2 gigabytes of memory, I'm actually allocating three gigabytes of data because everything is running on this same box."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741ee570",
   "metadata": {},
   "source": [
    "<img src=\"img/151.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a70c863a",
   "metadata": {},
   "source": [
    "Python will require memory to launch all the processes.\n",
    "\n",
    "MongoDB also runs on memory, so we also need some memory to be allocated, and then there's the per connection allocation that will be reflected on this increase.\n",
    "\n",
    "Once the system stops, after a minute, we go back to 1.2 gigabytes of utilization.\n",
    "\n",
    "Now if you're starting to wonder if this is a good thing or not let me reassure you one thing, this is not the normal behavior for a well written application.\n",
    "\n",
    "What we need to realize is that for each connection that we establish from our application or from the threads of our application, will require memory.\n",
    "\n",
    "And since everything in our current demonstration runs on the same box, it will be allocated on both sides, not only on the MongoDB side but also in the application side.\n",
    "\n",
    "So it's important to determine how many connections you allow your applications to establish considering that each connection require resources and when you have a lot of them, they can add up to a very big number in terms of resource requirements.\n",
    "\n",
    "But I'm feeling bold so let's not stick around with only 600, let's go ahead and try and push the envelope to 2000 connections and see what happens.\n",
    "\n",
    "Now obviously our memory keeps on increasing at the dramatic stage, and it would eventually be running into a couple of nasty situations.\n",
    "\n",
    "Like, for example, our processes not being able to be allocated because there is no memory left.\n",
    "\n",
    "This will be done on the application side.\n",
    "\n",
    "So next time we try to fork the process and launch a new process from our main script, the make lots of connection server, the OS will tell my applications a no you cannot do that go away and launches OS error saying that there is no memory available for being allocated.\n",
    "\n",
    "So if you don't manage it well the requirements in terms of your application resources, especially memory, and if you keep your connections for a long time without releasing them, regardless of the paralyzation that you want to do in your application, you might end up with problems.\n",
    "\n",
    "Where the server does not have any memory or that your own MongoDB will not be able to allocate more connections than what is feasible for that particular box.\n",
    "\n",
    "Now, there's a particular setting of your system that also interferes a lot with the number of connections that you will be able to set from your MongoDB and that's your ulimits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c218b4e1",
   "metadata": {},
   "source": [
    "<img src=\"img/152.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4875bdc2",
   "metadata": {},
   "source": [
    "Now MongoDB allows you by default to reach up to 65,536 connections per MongoDB if only your system is actually capable of doing so.\n",
    "\n",
    "If your ulimits value actually allows you to do so.\n",
    "\n",
    "Let's check out this particular box that we've been using the ulimits value.\n",
    "\n",
    "To do that, we just need ulimit-a, give us all the different ulimits that we can set up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55b652d",
   "metadata": {},
   "source": [
    "<img src=\"img/153.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "32797ea6",
   "metadata": {},
   "source": [
    "And in particular, one that is quite important, is my ulimits of open files.\n",
    "\n",
    "And here I can only open up to 1,024 different files in this box for this particular user.\n",
    "\n",
    "This value will impact the number of allowed connections and the number of collection and indexes that our system will be able to have.\n",
    "\n",
    "Since every single connection, index, and collection will be a file for MongoDB to operate with or will be considered a file that it needs to work with.\n",
    "\n",
    "We will reach a limit if we have a bunch of different connections plus a bunch of different collections and a bunch of different indexes operating under the same MongoDB, under the same user with this setting of ulimits.\n",
    "\n",
    "Now if I connect to my MongoDB and I look for my service status and in particular, the number of connections, I can see a couple different values here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64937fe0",
   "metadata": {},
   "source": [
    "<img src=\"img/154.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "53209c46",
   "metadata": {},
   "source": [
    "This MongoDB is allowed to establish up to 800 in 18 different connections.\n",
    "\n",
    "So the ulimits configuration will affect the number of available connections that your MongoDB will be actually allowing you to set regardless of the default maximum value and regardless of the number of incoming maximum connections that you set.\n",
    "\n",
    "Given that we have 1,024, my MongoDB will only have available 818 because there's a bunch of other files that MongoDB needs to operate with that also are going to be reducing the number of available connections.\n",
    "\n",
    "But then again, I think that 818 is a very small number.\n",
    "\n",
    "So what I'm going to do is shut down my server for a second, bring it down, I'm going to change my ulimit value.\n",
    "\n",
    "I'm going to set up the number of allowed open files to 2,048 duplicating this amount, I'm going to bring back my MongoDB, and connect to get into that instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a812fc",
   "metadata": {},
   "source": [
    "<img src=\"img/155.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "be0dbfd7",
   "metadata": {},
   "source": [
    "Once I connect it, If I do my server status again looking for the connections, I can see that my available connections are also immediately increased as a factor of the ulimits value.\n",
    "\n",
    "So keep track of your ulimits because this might affect a lot the amount of open files that a given process is able to establish and therefore, limit your connectivity of your system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8134cf00",
   "metadata": {},
   "source": [
    "# Closing and Dropping Connections-Election of new Primary"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ff877a4d",
   "metadata": {},
   "source": [
    "Another event that might cause connections to drop is the high availability mechanism itself, especially around elections.\n",
    "\n",
    "When we elect a new primary, something happens.\n",
    "\n",
    "And you should, at the very least, drop some corrections.\n",
    "\n",
    "Let's see about that.\n",
    "\n",
    "Replica sets exist to allow our application to remain active in case of server failure.\n",
    "\n",
    "Once our primary is disconnected, either for maintenance or due to unplanned system shut down, our application is capable of staying up because there's an election process in place where a new member of the node will become the primary.\n",
    "\n",
    "And the application will know who to connect to.\n",
    "\n",
    "Now, there is something that happens here that the application needs to be aware of.\n",
    "\n",
    "The previous primary is no longer available.\n",
    "\n",
    "So you should know about it and connect to the new primary.\n",
    "\n",
    "So let's see this in action.\n",
    "\n",
    "And let's simulate in an environment where we have a replica set to do that.\n",
    "\n",
    "First thing I'm going to do is create a folder called, conns replica set, or connsrepl, in this case, where I'm going to hold all the data of my replica nodes.\n",
    "\n",
    "Second step is to create the replica set using a launch.\n",
    "\n",
    "Then I'm going to name here, conns.\n",
    "\n",
    "And I'm going to store all that data in this newly created folder.\n",
    "\n",
    "On a separate tab, I can launch my mongostats with --discover to allow me to connect to a node and then discover all the replica set, or even shared member that are affected to this particular node that I'm connecting to.\n",
    "\n",
    "So here, once I connect to this node, I can see that there's a repl status here that allows me to know one of these particular nodes is a primary, another secondary, another secondary.\n",
    "\n",
    "Now what I'm going to do is pick up my script and create a connection to this replica set.\n",
    "\n",
    "So I'm seeing 600 different connections.\n",
    "\n",
    "So I can see that all my nodes are going to be connected by this script.\n",
    "\n",
    "If I go into the system and ask for, who is primary right now, I can see it's 27,000.\n",
    "\n",
    "Once I know who my primary is, which is 27,000, I will tell it to step down.\n",
    "\n",
    "So you will no longer become primary.\n",
    "\n",
    "You will relinquish that role to another member of the set.\n",
    "\n",
    "After we step down, we can see that, in our application, a few errors have been merged.\n",
    "\n",
    "Now what the system will tell us one of two things.\n",
    "\n",
    "Either it will not be able to complete the operation it's trying to do because this node is not master.\n",
    "\n",
    "So it will attempt to run an operation against the secondary.\n",
    "\n",
    "So the previous machine was a primary, no longer primary, needs to reconnect.\n",
    "\n",
    "But all in-flight operations will either get a not master error or will get an out to reconnect exception on your client application.\n",
    "\n",
    "By throwing you this not master error or the out to reconnect exception, is mode is basically telling you, hey, you are no longer connected to a primary.\n",
    "\n",
    "What do you want to do now?\n",
    "\n",
    "All subsequent connections, or instructions let's say, will go through no problem.\n",
    "\n",
    "Because the system will know who to reconnect after getting the first error message.\n",
    "\n",
    "So the driver will take care of that for you.\n",
    "\n",
    "For all in-flight operations, those will need to be decided by the application what to do with them.\n",
    "\n",
    "So look for those kind of error messages from in your code if you're a developer.\n",
    "\n",
    "Or if you are dba, please make sure your developers know about this and not assuming that mongodb will handle all things related with in-flight operations.\n",
    "\n",
    "Again, mongodb will not try to outsmart you.\n",
    "\n",
    "One thing that we could do is making our application aware of the exceptions by capturing those and doing something about it.\n",
    "\n",
    "In this case, we will just need to say, well I can handle this no problem since we are only reading.\n",
    "\n",
    "But if you imagine any write that could appear within this block of code, we will need to know what do we want to do about it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab37276",
   "metadata": {},
   "source": [
    "<img src=\"img/156.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a8c1f4f9",
   "metadata": {},
   "source": [
    "Do we want to retry the operation?\n",
    "\n",
    "Do we just want to discard it and try it later?\n",
    "\n",
    "Or are we just not going to be dealing with that?\n",
    "\n",
    "Mongodb will not be smarter than you on those situations.\n",
    "\n",
    "It will let you decide what you want to do.\n",
    "\n",
    "Keep that in mind for your application building in a future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e2bf79",
   "metadata": {},
   "source": [
    "### 範例01"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3e5351c5",
   "metadata": {},
   "source": [
    "Which of the following will affect the number of available incoming connections a mongod instance will allow to be established?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c02bc035",
   "metadata": {},
   "source": [
    "The following are true:\n",
    "\n",
    "# OS ulimits\n",
    "The number of allowed open files will affect the number of connections that a mongod will be allowed to establish\n",
    "\n",
    "# net.maxIncomingConnections configuration file parameter\n",
    "This configuration option will allow the system administrator to determine a max number of incoming connections\n",
    "\n",
    "This is false:\n",
    "\n",
    "# 65536 is the max number of connections, no other limit will apply\n",
    "Obviously, this can't be true, since the others are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372b3dec",
   "metadata": {},
   "source": [
    "# Write Concern and Timeouts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903389ab",
   "metadata": {},
   "source": [
    "<img src=\"img/157.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "04840d5c",
   "metadata": {},
   "source": [
    "OK, let's talk about something that doesn't come up often, but which does come up.\n",
    "\n",
    "The interaction between write concern and throughput.\n",
    "\n",
    "When using write concern, which you almost certainly should do, the server must acknowledge each write back to the driver.\n",
    "\n",
    "That acknowledgement can be delayed, or otherwise not returned, for any number of reasons, but it's possible to create cluster architectures that can outright prevent acknowledgement when even a single server goes down.\n",
    "\n",
    "It's also possible to set things up so that reasonable delays in acknowledgement result in errors.\n",
    "\n",
    "Let's take a look.\n",
    "\n",
    "In write concern, we have a W parameter.\n",
    "\n",
    "It specifies how many servers need to perform the write before it gets acknowledged.\n",
    "\n",
    "In write concern, we have this W parameter.\n",
    "\n",
    "It specifies how many servers need to perform the write before it gets acknowledged.\n",
    "\n",
    "The default write concern is w: 1.\n",
    "\n",
    "This means that the driver receives acknowledgement from the primary once it, the primary, has performed the write.\n",
    "\n",
    "This is acceptable for some use cases, but it's mathematically possible for that server to go down before the write has made it to any other servers.\n",
    "\n",
    "In cases where the application needs to be robust against any single point of failure, it's necessary to use a write concern of two or more.\n",
    "\n",
    "Under almost all such cases, the preferred write concern is majority.\n",
    "\n",
    "This ensures that acknowledged writes will be durable, except in cases where you lose a majority of your servers.\n",
    "\n",
    "Note that I'm being very precise about my language.\n",
    "\n",
    "It doesn't necessarily guarantee that your writes will get acknowledged, even if your primary, which requires a majority of votes, is up.\n",
    "\n",
    "At least not for all architectures, as we'll see.\n",
    "\n",
    "By the way, I'm going to focus on write concern, but this applies to read concern as well, for architectures that use it.\n",
    "\n",
    "Let's dig in a little deeper.\n",
    "\n",
    "Starting with w: 1.\n",
    "\n",
    "First, our driver sends out a write, then the server performs the write, but the acknowledgement hasn't yet been sent.\n",
    "\n",
    "This situation doesn't last long, because the acknowledgement is sent back as fast as the server can make it happen.\n",
    "\n",
    "When acknowledgment occurs, the data could be in this state written to just your primary, or perhaps it's made it to one of the secondaries by the time it gets acknowledged.\n",
    "\n",
    "I'm just showing you all of the possibilities for completeness.\n",
    "\n",
    "Now, what if the write concern is majority?\n",
    "\n",
    "Now things get a little more interesting.\n",
    "\n",
    "For a three member replica set, a majority means two copies of the data, each on different servers.\n",
    "\n",
    "So a write has to propagate beyond the primary, and then that fact has to get communicated back to the primary before it can be acknowledged.\n",
    "\n",
    "So, after we write to the primary, the secondary performs the write, then the secondary lets the primary know it's got the write, and then the primary can acknowledge that write to the driver.\n",
    "\n",
    "Obviously, it could go to the other secondary first, or to both.\n",
    "\n",
    "Now what if there's a partition?\n",
    "\n",
    "It still works fine, because it has the other secondary.\n",
    "\n",
    "So, a network partition, or a secondary going down, won't cause much of a problem, in and of itself, for this vanilla setup.\n",
    "\n",
    "Obviously, things get even more complicated when you're dealing with, say, a five member replica set, and a write concern of majority, but we're not going to go into that level of detail here.\n",
    "\n",
    "OK, so that's a description of how things generally work in vanilla replica sets.\n",
    "\n",
    "Now we'll talk about what can go wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d04dca",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5dce2022",
   "metadata": {},
   "source": [
    "OK.\n",
    "\n",
    "So we've seen how vanilla replica sets usually work with the write concern of majority.\n",
    "\n",
    "With that in mind, what could possibly go wrong?\n",
    "\n",
    "Not much unless we switch out a secondary for an arbiter.\n",
    "\n",
    "Now I've only got one path to get my data acknowledged.\n",
    "\n",
    "This still works, but only as long as nothing goes wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619e2e5d",
   "metadata": {},
   "source": [
    "<img src=\"img/158.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "39ce8e35",
   "metadata": {},
   "source": [
    "If a partition occurs between my primary and secondary then my primary can't acknowledge any writes until the partition is resolved.\n",
    "\n",
    "Similarly, if that secondary goes down then my writes won't get acknowledged until that secondary's back up.\n",
    "\n",
    "Keep this in mind, a replica set with an arbiter in place of a data bearing server is better than an even number of replica set members but it can also cause bad situations when anything else goes wrong.\n",
    "\n",
    "Let's go back to the situation with three data bearing members but ask what happens with a delayed secondary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de75636",
   "metadata": {},
   "source": [
    "<img src=\"img/159.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf1085ed",
   "metadata": {},
   "source": [
    "A delayed secondary is only about as good as an arbiter for purposes of ensuring that your W majority writes go through.\n",
    "\n",
    "That's because you need to add your delay to any time for a write to occur and get acknowledged.\n",
    "\n",
    "So if a secondary is delayed by five hours then that secondary won't acknowledge any of the writes for 5 hours, which will be a problem if my theoretically up-to-date secondary goes down.\n",
    "\n",
    "Of course you're always better off with a larger replica set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886dab18",
   "metadata": {},
   "source": [
    "<img src=\"img/160.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "78f8fce9",
   "metadata": {},
   "source": [
    "So this brings up an interesting point.\n",
    "\n",
    "You should always go through the routine of calculating how many servers you need at minimum for a majority."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9ad0a0",
   "metadata": {},
   "source": [
    "<img src=\"img/161.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1eb5142b",
   "metadata": {},
   "source": [
    "Remember, this is about write concern not your ability to elect a primary.\n",
    "\n",
    "Don't count an arbiter if you have one, since it can't hold data.\n",
    "\n",
    "Don't count delayed secondaries.\n",
    "\n",
    "You want to know how many servers you need to write to before you can get that acknowledged.\n",
    "\n",
    "And you want to know how many of them can be out of the picture either because they're down or because of network partitions.\n",
    "\n",
    "Then figure out how robust your system is to your standard servers going down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7851ea",
   "metadata": {},
   "source": [
    "# Write Concern and Timeouts-Demo"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d1d5d03",
   "metadata": {},
   "source": [
    "We've been drawing diagrams up until this point.\n",
    "\n",
    "Let's see all of this in action.\n",
    "\n",
    "I'll spin up a replica set with three members in my VM.\n",
    "\n",
    "Next, I'll run a script I wrote to test write concern.\n",
    "\n",
    "It performs a set of writes, 10 by default, with a write concern I can specify using majority by default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a29b30",
   "metadata": {},
   "source": [
    "<img src='img/162.png'>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7aae4d61",
   "metadata": {},
   "source": [
    "Let's run 100 tests with write concern majority, and see what the average time to get our write acknowledged is.\n",
    "\n",
    "That took an average time of 2.6 milliseconds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e816fb2",
   "metadata": {},
   "source": [
    "<img src=\"img/163.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c5fb59f6",
   "metadata": {},
   "source": [
    "Pretty fast, but that makes sense; I'm just doing this on localhost.\n",
    "\n",
    "Now let's see what happens when I try to do this with a write concern of four.\n",
    "\n",
    "So that's good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff16d58",
   "metadata": {},
   "source": [
    "<img src=\"img/164.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc626b18",
   "metadata": {},
   "source": [
    "If I try to use a write concern of four, then it will throw this write concern error -- Not enough data-bearing nodes.\n",
    "\n",
    "So it knows that when I have a three member replica set, but want a write concern of four, that I'm asking for the impossible, and it will just tell me no.\n",
    "\n",
    "The problem is that it's not as obvious to your system if one of your servers is just down.\n",
    "\n",
    "I'll disconnect, and run my script again, this time with a write concern of three and a wTimeOut of one second."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7951519f",
   "metadata": {},
   "source": [
    "<img src=\"img/165.png\">\n",
    "<img src=\"img/166.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8fea0a55",
   "metadata": {},
   "source": [
    "And as I can see it timed out after that one second delay.\n",
    "\n",
    "So, this wTimeOut will let my replica set know that it should throw an error if it takes more than a second to replicate the data, in this case to the three members since W equals three.\n",
    "\n",
    "Be aware that, if you don't have that timeout set, my write concern won't even get that error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a4210b",
   "metadata": {},
   "source": [
    "<img src=\"img/167.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c2d08ac0",
   "metadata": {},
   "source": [
    "And it just hangs, because by default, there is no wTimeOut.\n",
    "\n",
    "The application won't move on without acknowledgment, but it will wait for that acknowledgement for as long as it takes.\n",
    "\n",
    "I'm going to use control-C to kill this.\n",
    "\n",
    "And just to be clear, the write occurred, it's just not getting to all three servers as specified in my write concern.\n",
    "\n",
    "So, it's definitely important to use a wTimeOut, but as we'll see, we don't want to set it too low either.\n",
    "\n",
    "Let's use mlaunch to shut down my two remaining servers, and then restart the whole set.\n",
    "\n",
    "Now, let's see how long things take for a write concern of three."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800f3537",
   "metadata": {},
   "source": [
    "<img src=\"img/168.png\">\n",
    "<img src=\"img/169.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c4d80e63",
   "metadata": {},
   "source": [
    "And let's see what happens if I set my wTimeOut to 1 millisecond.\n",
    "\n",
    "Even on local host, it can take more than one millisecond in order for a write to propagate.\n",
    "\n",
    "Keep in mind, this script uses W majority by default, so, for a three member replica set, that's the same as W two.\n",
    "\n",
    "For a wTimeOut of 2 milliseconds, it looks like some of them went through, and then one of them failed on this machine.\n",
    "\n",
    "Your results may differ.\n",
    "\n",
    "What if I had a replica set with two nodes plus an arbiter?\n",
    "\n",
    "Let's change our configuration and find out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3ac769",
   "metadata": {},
   "source": [
    "<img src=\"img/170.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7c3a1a68",
   "metadata": {},
   "source": [
    "And let's connect.\n",
    "\n",
    "When I look at my rs.conf, I can see that my second member has this arbiter only flag set.\n",
    "\n",
    "That means it's an arbiter, it's not carrying data.\n",
    "\n",
    "Now what if a member goes down?\n",
    "\n",
    "I've killed my secondary.\n",
    "\n",
    "Let's run this with W majority by default.\n",
    "\n",
    "I lost just one server of what I thought of as a three member replica set, and what my write concern thinks of as a three member replica set.\n",
    "\n",
    "I still have a primary, but it can't handle W majority at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386445f5",
   "metadata": {},
   "source": [
    "<img src=\"img/171.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b5f5aad4",
   "metadata": {},
   "source": [
    "So what have we talked about?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961d92ad",
   "metadata": {},
   "source": [
    "<img src=\"img/172.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "92740483",
   "metadata": {},
   "source": [
    "We've discussed how arbiters and delayed secondaries can cause problems with write concerns of greater than one.\n",
    "\n",
    "We've talked about the importance of being mindful of your majority, and we've seen how this plays out in action with wTimeOut.\n",
    "\n",
    "And that's a quick tour of unexpected problems that can arise from write concern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcf2365",
   "metadata": {},
   "source": [
    "### 範例01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a966c746",
   "metadata": {},
   "source": [
    "For a 7-member replica set with one arbiter and one delayed secondary, how many members are needed in order to acknowledge { w : \"majority\" } before the wtimeout is hit?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "30d3732a",
   "metadata": {},
   "source": [
    "For a 7-member replica set, the answer is always 4. The arbiter and the delayed secondary don't change that fact.\n",
    "\n",
    "The danger is that, if there's both an arbiter and a delayed secondary, there are only 5 other servers that are providing availability. If two data-bearing servers go down, your replica set will be able to elect a primary, but will not be able to acknowledge writes with { w : majority }."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c75f38",
   "metadata": {},
   "source": [
    "# Hostnames and Cluster Configuration"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bc07ac11",
   "metadata": {},
   "source": [
    "MongoDB runs on a distributed cluster.\n",
    "\n",
    "A replica set is composed by different hosts, and inside those hosts we're going to have instances of mongod that belong to a replica set.\n",
    "\n",
    "The same applies to sharded cluster which also can be composed of different nodes.\n",
    "\n",
    "Now any of these hosts might have different ways by which you can reach them.\n",
    "\n",
    "They can have different NICs, network interface controllers, or network interface cards.\n",
    "\n",
    "They can have different DNS names provided by a DNS server inside of your network.\n",
    "\n",
    "Or it also can have different established, fully qualified domain names also established by your DNS, or even just configured on your host files, a manual to find list of addresses that will contain which IP address complies with each machine name.\n",
    "\n",
    "Now once we have a configuration of a replica set where one of the hosts is a primary and then the following ones are secondaries, if we don't configure things well given this set of different names by which we can reach the machines and the different interface cards that we might have in our different hosts, things can get a little messy.\n",
    "\n",
    "And in this lesson, we will be looking to how to get us out of a messy situation, and how to identify clusters which are poorly configured given the different ways that we have to reach to those machines.\n",
    "\n",
    "To help us out here with the handouts of this lesson, you will find this nodes vagrant environment.\n",
    "\n",
    "So go ahead.\n",
    "\n",
    "I generally put it on some folder like this, where I have my home folder, university, the course I'm in, and obviously all the handouts and material that were made available.\n",
    "\n",
    "So in this environment, what I'm going to have is three different machines, three different virtual machines, each one of them having a mongod in them, and they all belong to the same replica set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e9d197",
   "metadata": {},
   "source": [
    "<img src=\"img/173.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "05bc4db7",
   "metadata": {},
   "source": [
    "If I connect one of the machines and ask for the replica set status, I can see the fully configured three-node configuration of that replica set.\n",
    "\n",
    "There will be a secondary, a primary, and finally another secondary.\n",
    "\n",
    "Now the way that I'm connecting to this machine is by simply knowing an IP address of one of the hosts and connecting to the default port."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcb3d38",
   "metadata": {},
   "source": [
    "<img src=\"img/174.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "53710f92",
   "metadata": {},
   "source": [
    "Here we'll just ask for the rs.status, and that will give me the status of the nodes that belong to the replica set, which include this particular machine that I know of.\n",
    "\n",
    "But I also know that this particular node belongs to M312, the name of the replica set.\n",
    "\n",
    "So a good way of connecting to this machine, or a proper way to connect to this machine, is by appending the name of the replica set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b15d2a",
   "metadata": {},
   "source": [
    "<img src=\"img/175.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3b4935d8",
   "metadata": {},
   "source": [
    "Therefore I will have the list of different nodes that belong to the replica set, and I'll be able to connect directly to the primary machine of that particular set.\n",
    "\n",
    "So if I do so, I end up in a bad state.\n",
    "\n",
    "Here my local machine, my local laptop, is trying to connect to a virtual machine that basically doesn't know who it is.\n",
    "\n",
    "I am able to connect to one of the members by a [?\n",
    "\n",
    "cell's ?] direct connection.\n",
    "\n",
    "But once I want to connect using the replicas set, since I do not know or I cannot reach by the name defined in the replica set, the primary node, I cannot establish it during connection.\n",
    "\n",
    "If I look into the replica set status again, and if I look for the configuration of my primary, I will see that my primary node is configured using m2.university.mongodb at port 27017.\n",
    "\n",
    "For me to understand if I can reach this machine or not, by simply pinging the box I would be able to check that I do not in fact know who this node is.\n",
    "\n",
    "And that will bring us to the important question here.\n",
    "\n",
    "How well should my local machine be connected to that replica set in place?\n",
    "\n",
    "Do I need just one node or should I know all of the nodes of the replica set in its own?\n",
    "\n",
    "Because if I connect directly to the node that I know of, which in my case is my m1 box, and if I tried to connect to my replica set, I'm able to connect with no frills.\n",
    "\n",
    "System connects, all things worked well.\n",
    "\n",
    "If I use the machine itself, my m1 itself, I'm still able to be correctly redirected to the primary, which in this case is my m2 university.mongodb.\n",
    "\n",
    "If I want one application to be able to connect correctly to this replica set in place, it should be able to connect to all configured nodes of the replica set itself, which in this particular case are configured by their simple name, m3, by their fully qualified name, m2 university mongodb, and even by their own IP address, in this case will be m1.\n",
    "\n",
    "Now as you can see here, these three different ways which this host can be reached can cause problems for your applications to connect to.\n",
    "\n",
    "Because if one of the nodes would eventually be primary and I would not know how to reach them, my application will not be able to connect to him while a failover is taking place or a new election is taking place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15241887",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b5af6cca",
   "metadata": {},
   "source": [
    "Obviously a way to fix this would be to edit my hosts file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca56499",
   "metadata": {},
   "source": [
    "<img src=\"img/176.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "00edb530",
   "metadata": {},
   "source": [
    "In this case, in my local machine, where I have my application, which in this case is just my mongo shell.\n",
    "\n",
    "But it will serve the same needs.\n",
    "\n",
    "I will just need to make sure that it knows how to identify m2.university.mongodb.\n",
    "\n",
    "And also m2 because this is also something that the machine will need to know in the future.\n",
    "\n",
    "But would that be enough?\n",
    "\n",
    "Would just knowing one of the nodes, or two of the nodes in this case, and by the names and configuration details that are configured in the replica set definition.\n",
    "\n",
    "Would that be enough?\n",
    "\n",
    "Well let's try it out.\n",
    "\n",
    "Now I know I can reach the machine because I configured my hosts file that responds to m2.university.mongodb.\n",
    "\n",
    "Let's test it out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c9c388",
   "metadata": {},
   "source": [
    "<img src=\"img/177.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "47807870",
   "metadata": {},
   "source": [
    "Yes I can.\n",
    "\n",
    "Here we are.\n",
    "\n",
    "We can connect.\n",
    "\n",
    "Everything is great."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de37551f",
   "metadata": {},
   "source": [
    "<img src=\"img/178.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d3d91d9",
   "metadata": {},
   "source": [
    "Can I reach it through, for the previous IP address my m1 IP address, which is also part of the configuration of my replica set.\n",
    "\n",
    "In this case, I also can do so because both of those IP addresses are known.\n",
    "\n",
    "But now let's see what happens when the configuration changes.\n",
    "\n",
    "So essentially what we were setting up in the previous configuration was that, although our replica set is fully well configured, in this case using the IP address, in that other case using the fully qualified name, and this one using m3, I would not know what m2 and its fully qualified name would be looking like.\n",
    "\n",
    "So it would effectively be out of reach for those.\n",
    "\n",
    "And once that machine will be brought down, if was a primary or not, since I would not be able to reach those machines, I could not failover to the existing nodes, defeating the purpose of having multiple nodes for high availability.\n",
    "\n",
    "If I only can reach one, if I cannot reach the other members of the node, then I don't have any availability whatsoever.\n",
    "\n",
    "Once I do add the second member by adding my fully qualified name of m2 and if I express my mongodb URI, specifying more than one member that I know of, if one of them is down it will try the other one to connect.\n",
    "\n",
    "And if it can understand what it is, and how it's reachable from the machines, and if it's configured properly, I will be able to establish a failover operation to that new node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23477ce",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "id": "85dea9f1",
   "metadata": {},
   "source": [
    "One of the other things that can happen is, although one or two machines can reach each other, there might be a misconfiguration between two pair of nodes in your replica set.\n",
    "\n",
    "Let's see this in action.\n",
    "\n",
    "Let's go ahead and connect to my m1 box.\n",
    "\n",
    "And let's do something weird into the configuration.\n",
    "\n",
    "Let's say to the system that, instead of having m3, I only know the machine by its fully-qualified name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b839db5",
   "metadata": {},
   "source": [
    "<img src=\"img/179.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f1e94887",
   "metadata": {},
   "source": [
    "Once I do this-- and if I connect to the running host-- if I get a rs.status, I will see that my configuration are all up and running perfectly well.\n",
    "\n",
    "So there's a majority of nodes that know about each other.\n",
    "\n",
    "Notice how I connect to the machine using the replica set.\n",
    "\n",
    "I'll get an information warning stating, well, connecting to this box, getting address of m3 failed.\n",
    "\n",
    "No name or service known to the machine that I'm connecting.\n",
    "\n",
    "This is from my own m1 box, from whom we removed the definition m3.\n",
    "\n",
    "We just keep the fully-qualified name here.\n",
    "\n",
    "And once we try to connect to [?\n",
    "\n",
    "ourselves ?] replica set, we will see that there's an error message, or a warning message, saying, hey, watch out I do not know who m3 is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4f7260",
   "metadata": {},
   "source": [
    "<img src=\"img/180.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a5c8980e",
   "metadata": {},
   "source": [
    "So look for errors like those to make sure that all of your nodes can reach each other and know about each other, regardless of the way that we configured them.\n",
    "\n",
    "And obviously, if there's an election in place, like if we tell m2 to step down, and relinquish the primary role, if we connect once more, and if m3 is not reachable by this local machine, we will get into a state where there's no way we can elect a primary.\n",
    "\n",
    "So basically, we end up in the situation where our primary gets disconnected, or steps down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254838ee",
   "metadata": {},
   "source": [
    "<img src=\"img/181.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c405986e",
   "metadata": {},
   "source": [
    "We have a secondary that can be reached by this old primary.\n",
    "\n",
    "He knows about m3.\n",
    "\n",
    "But this configured machine does not know who M3 is.\n",
    "\n",
    "So ending up in a situation where we cannot elect, because this node will not know who that one is.\n",
    "\n",
    "Let's avoid doing that.\n",
    "\n",
    "So let's fix the order of the world, making sure that all nodes know about each other, and they are aware of each other's IP addresses.\n",
    "\n",
    "But if you think about it, for example, in a cloud environment, where machines can be brought down, and if we configure them with their IP addresses.\n",
    "\n",
    "And since that can be renewed, you can get into a messy state with machines that are in the sets can be reached, but not by their current IP address, or not by their old DNS name, because they cannot be identified exactly in the same way they are configured in their replicas and configuration.\n",
    "\n",
    "So we should have some good practices in place, or some best networking practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b725ab",
   "metadata": {},
   "source": [
    "<img src=\"img/182.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "720cb3e1",
   "metadata": {},
   "source": [
    "All right.\n",
    "\n",
    "Because that nodes should be addressable from the client host, and other replica set hosts.\n",
    "\n",
    "All mongoses in a cluster should also be addressable by all client applications into all replica sets.\n",
    "\n",
    "We urge you to use DNS names instead of IP addresses to avoid IP address renew resolution conflicts.\n",
    "\n",
    "If we define good DNS names to our machines, it will be easier to move those nodes into other different hardware physical boxes, which is OK.\n",
    "\n",
    "If we use fixed IP addresses, it might be more tricky to move those machines around.\n",
    "\n",
    "And every time we renew the IP address of a node, we might lose accessibility and readability from that node, causing internal problems in terms of configuration.\n",
    "\n",
    "Make sure you use ping and other networking commands to reach machines, and to make sure that they can reach each other.\n",
    "\n",
    "And use telnet as well to see if the ports that we are using are reachable, and they are properly configured.\n",
    "\n",
    "That's all we have for you in terms of making sure your hosts and cluster configurations match, and are well set up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43ef214",
   "metadata": {},
   "source": [
    "### 範例01"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8450185d",
   "metadata": {},
   "source": [
    "When connecting to a MongoDB Cluster, I should guarantee that:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6916c89d",
   "metadata": {},
   "source": [
    "The following are true:\n",
    "\n",
    "# All mongos's and replica set members are addressable by the client and application hosts\n",
    "# The application should use more than one replica set member in the connection string\n",
    "Confirming these can avoid problems with hostnames later on.\n",
    "\n",
    "The following is false:\n",
    "\n",
    "# We can set any NIC or hostname for our replica set configuration set, since MongoDB will figure things out for us.\n",
    "Definitely not the case. MongoDB will use the hostnames given, which may seem to work fine until a part of the cluster (or the application) can no longer rely on its hostnames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e472312",
   "metadata": {},
   "source": [
    "# Sharding Issues"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b754d7b4",
   "metadata": {},
   "source": [
    "OK.\n",
    "\n",
    "Let's talk about some issues that can arise in the context of sharding.\n",
    "\n",
    "The most obvious thing to keep in mind is that after MongoDB 3.4, you can have up to one migration per pair of shards, one sourcing the chunk and one receiving it.\n",
    "\n",
    "Here, we can see exactly that happening.\n",
    "\n",
    "And, if you have six shards, you could have as many as three chunks in flight.\n",
    "\n",
    "If three of your servers have too many chunks and the other three have too few, one problem that can arise is if you end up getting jumbo chunks.\n",
    "\n",
    "If a chunk grows large enough without being split, it becomes a jumbo chunk.\n",
    "\n",
    "The sharded cluster won't try to move it as the chunk could be in flight for too long.\n",
    "\n",
    "MongoDB will also stop trying to split the chunk.\n",
    "\n",
    "Jumbo chunks often come from having a shard key with insufficient cardinality, meaning that there is no shard key value it can split on.\n",
    "\n",
    "By default, in a sharded cluster, the chunks should be split at 64 megabytes.\n",
    "\n",
    "Let's do an example.\n",
    "\n",
    "We're setting up a sharded cluster with three shards.\n",
    "\n",
    "Each shard is a replica set with three members.\n",
    "\n",
    "Great.\n",
    "\n",
    "Let's connect and shard a collection.\n",
    "\n",
    "We'll enable sharding and shard on last name comma first name for the example collection.\n",
    "\n",
    "Great!\n",
    "\n",
    "Let's look at sh.status.\n",
    "\n",
    "So all of my data, such as it is, is in a single chunk but that chunk is empty.\n",
    "\n",
    "Let's insert a bunch of documents all with the same shard key and create a jumbo chunk.\n",
    "\n",
    "I'm going to make the documents one megabyte each by creating a string that's one megabyte in length.\n",
    "\n",
    "OK.\n",
    "\n",
    "So all of my data is now in a single chunk and it can't be split because it all has the same shard key values.\n",
    "\n",
    "Let's look at sh.status.\n",
    "\n",
    "There it is, one chunk.\n",
    "\n",
    "Obviously, this is a bad shard key if I can do this, but let's deal with some of the fallout.\n",
    "\n",
    "We'll try adding some more data, this time on a different value.\n",
    "\n",
    "We'll use the name Norberto Leite.\n",
    "\n",
    "OK.\n",
    "\n",
    "So now I have 200 documents of about a megabyte each and if I look at sh.status, I can see that I've still only got one chunk on one shard.\n",
    "\n",
    "All of my reads and writes are going to hit this one shard and none of them will hit either of the other shards, so I've got an imbalance.\n",
    "\n",
    "Let's see what's going on in our server logs.\n",
    "\n",
    "OK.\n",
    "\n",
    "I can see the sharded cluster is starting out, request split points.\n",
    "\n",
    "If I scroll down a bit, request it, possible low cardinality key detected in m312.example, key is last name Cross, first name Will.\n",
    "\n",
    "OK.\n",
    "\n",
    "So the server already knows I have a bad shard key.\n",
    "\n",
    "Now I've got a couple of choices here, I can try to manually split the chunks, say with sh.splitAt or I can increase the chunk size.\n",
    "\n",
    "First, I'm going to split it manually.\n",
    "\n",
    "Great.\n",
    "\n",
    "Let's look at sh.status.\n",
    "\n",
    "OK.\n",
    "\n",
    "So now I have two chunks, each of them are 100 megabytes and I'm still not balancing them.\n",
    "\n",
    "They're both just staying on my one shard.\n",
    "\n",
    "To fix this issue, I'm going to try to move a chunk manually.\n",
    "\n",
    "And MongoDB is telling me no, the chunk is too big.\n",
    "\n",
    "OK.\n",
    "\n",
    "Let's try to increase the chunk size.\n",
    "\n",
    "We can do this by modifying the chunk size directly.\n",
    "\n",
    "Usually, when we do things like this, we would use a helper function but as of this recording, we need to actually modify the configdb.\n",
    "\n",
    "Something that isn't to be done lightly but it is necessary in this case.\n",
    "\n",
    "I'm going to set the value at 150 megabytes.\n",
    "\n",
    "Good.\n",
    "\n",
    "Once again, let's look at sh.status.\n",
    "\n",
    "My chunks still haven't migrated.\n",
    "\n",
    "Let's do one more split to wake it up and get it going.\n",
    "\n",
    "And we'll look at sh.status.\n",
    "\n",
    "OK.\n",
    "\n",
    "And if I do that again, great, three chunks and they're distributed.\n",
    "\n",
    "Now just to be clear, one of those chunks contains no data and I was able to migrate my jumbo chunks by changing my chunk size.\n",
    "\n",
    "This isn't going to help if I have a bad shard key like I do here because I'll just continue adding documents to the shard key values, but that's something you can do in cases where you get stuck.\n",
    "\n",
    "Now, let's create a bunch of chunks and let the cluster distribute them.\n",
    "\n",
    "Sh.status Whole lot of chunks and it's distributing them.\n",
    "\n",
    "And after a few moments, I have a nicely balanced cluster with 10, 9, and 9 chunks respectively on each of those shards.\n",
    "\n",
    "But of course, my data isn't evenly distributed.\n",
    "\n",
    "There's a great helper function in the shell to assist us with looking at our data distribution.\n",
    "\n",
    "It's called db.collection.getshard distribution.\n",
    "\n",
    "Let's check it out.\n",
    "\n",
    "OK, so what am I looking at?\n",
    "\n",
    "Shard02 seems to have 200 megabytes, shard03 none, total 200 megabytes, 200 chunks, there you go.\n",
    "\n",
    "Shard02 contains 100% of the data in the cluster.\n",
    "\n",
    "I can use this information to figure out if I need to split chunks on my own, figure it out if I've got a problem with my shard key, etc.\n",
    "\n",
    "And that's how to look for and fix some basic issues in a sharded cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df74a5c1",
   "metadata": {},
   "source": [
    "### 範例01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220abec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Assuming the default chunk size, which of the following is/are jumbo chunks?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6591984c",
   "metadata": {},
   "source": [
    "The default chunk size is 64 MB. 50 MB is below this, so all choices below and including 50 MB were wrong. The answers 100 MB and above are correct. You could see in the video that the balancer wouldn't move these until we changed the chunk size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae1fc70",
   "metadata": {},
   "source": [
    "### 範例02-Diagnosing unknown issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b48a91b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
