{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Concerns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In this lesson we're going to discuss read concerns.\n",
    "\n",
    "- Represent different levels of \"read isolation\"\n",
    "- Can be used to specify a consistent view of the database"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Read concerns are similar to write concerns in that they both involve how many nodes have applied a database operation.\n",
    "\n",
    "While write concerns affected the acknowledgement received by the driver after a write operation, read concerns affect the data returned by a read operation. Different read concerns are referred to as different levels of \"read isolation,\" because you can essentially \"isolate\" a read from the rest of the database if the data being read has only been written to one of the nodes. If data can be read by clients before that data has been replicated to a majority of nodes, it's considered a low level of read isolation.\n",
    "\n",
    "The read concern you choose will depend on how consistent your view of the database needs to be. If you can afford to read slightly stale data, then maybe a low level of read isolation might suit your application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3.amazonaws.com/edu-static.mongodb.com/lessons/M220/notebook_assets/replica_set_read_concern_local.png\" style=\"margin: 0 auto;\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "By default, when an application sends a read to a replica set, Mongo uses readConcern(\"local\") (point). From the perspective of the database client (point), the data read using readConcern(\"local\") has only been written to this one (point) node. In the vast majority of cases, the data will also be written to the other nodes (point) in the set, but the client only has proof that this (point) one node applied the write.\n",
    "\n",
    "This means that there's a chance, however slim, that the data (point) returned from this read will be rolled back. This would happen if, sometime after this (point) data is returned and before the secondaries have replicated that data, the primary goes down and a secondary gets elected to become the new primary. That would mean that one of these (point) two nodes, who haven't replicated the data yet, will be the new primary, and the old primary will be rolled back to match the state of the new primary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3.amazonaws.com/edu-static.mongodb.com/lessons/M220/notebook_assets/replica_set_read_concern_majority.png\" style=\"margin: 0 auto;\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "So the default readConcern in MongoDB is \"local\", which reads whatever copy of the data exists on this (point) node, regardless of whether or not the other nodes had replicated that data. And for the vast majority of reads, readConcern(\"local\") will work just fine.\n",
    "\n",
    "But we might want a higher level of consistency on some of our reads, which we can achieve with the readConcern called \"majority\". When the database client sends a read to Mongo with readConcern(\"majority\"), it can verifiably claim that the data it gets back (point) has been replicated to a majority of nodes the replica set. The benefit of this readConcern level is that once data has been replicated to a majority of nodes, it's super durable in the event of a failure. Even if the current primary (point) fails, this (point) secondary can be elected primary and then the data won't get rolled back.\n",
    "\n",
    "One thing to note here: if the secondaries aren't done replicating data at the time that the primary receives this (point) write, then whatever copy of the data has been replicated to a majority of nodes in the set will be the data returned to the client.\n",
    "\n",
    "This means that if my age (point) on the primary (point) is 66, but both of the secondaries still think that my age is 65, then the age 65 will be returned to the client. That's because in a replica set with three members, two nodes are required to constitute a majority.\n",
    "\n",
    "So clearly, readConcern(\"majority\") might return slightly stale data, but it provides a higher level of read isolation, so you can be more confident that the data you're reading won't get rolled back. For this reason it's most useful when reading mission-critical data, because lower levels of read isolation have a slightly higher chance of being rolled back in the event of an emergency. If your application's core functionality depends on one read, like checking a user's current account balance, then you probably want that read to have a higher durability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 範例01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "記得 from pymongo.read_concern import ReadConcern"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def most_active_commenters():\n",
    "    group = {\n",
    "        \"$group\":{\n",
    "            \"_id\": \"$email\",\n",
    "            \"count\": {\"$sum\": 1}\n",
    "        }\n",
    "    }\n",
    "    sort = { \"$sort\": {\"count\": -1} }\n",
    "    limit = { \"$limit\": 20}\n",
    "    pipeline = [group, sort, limit]\n",
    "\n",
    "    # we used Read Concern \"majority\" to make sure the data we read has been\n",
    "    # majority-committed\n",
    "    rc = ReadConcern(\"majority\")\n",
    "    comments = db.comments.with_options(read_concern=rc)\n",
    "    result = comments.aggregate(pipeline)\n",
    "    return list(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The default read concern in MongoDB is \"local\"\n",
    " - This does not check that data has been replicated\n",
    "The read concern \"majority\" allows for more durable reads\n",
    " - This only returns data that has been replicated to a majority of nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bulk Writes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In this lesson we're going to discuss a different type of writes called \"Bulk Writes,\" and the performance implications of these kinds of writes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3.amazonaws.com/edu-static.mongodb.com/lessons/M220/notebook_assets/replica_set_many_writes.png\" style=\"margin: 0 auto;\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Often times, our applications will encounter situations in which they need to perform a series of writes at once, and in some cases these writes have a causal effect on one another. One failing or succeeding in a group of operations, may affect your application logic.\n",
    "\n",
    "In this case, a user is purchasing food items on our grocery store application, and we are updating the database to reflect the new quantities we have in stock. This person bought 2 (point) apples, so we decrement the quantity by 2.\n",
    "\n",
    "When our application receives these writes, one option it has is to send each (point) of these writes (point) one at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3.amazonaws.com/edu-static.mongodb.com/lessons/M220/notebook_assets/replica_set_first_update_one.png\" style=\"margin: 0 auto;\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "So the client sends a write (point) over to the database (point)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3.amazonaws.com/edu-static.mongodb.com/lessons/M220/notebook_assets/replica_set_first_update_one_ack.png\" style=\"margin: 0 auto;\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "And at some point later, the client receives an acknowledgement that the write succeeded. Nice! Now let's send over the next write."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3.amazonaws.com/edu-static.mongodb.com/lessons/M220/notebook_assets/replica_set_second_update_one.png\" style=\"margin: 0 auto;\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "So we send over our next write,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3.amazonaws.com/edu-static.mongodb.com/lessons/M220/notebook_assets/replica_set_second_update_one_ack.png\" style=\"margin: 0 auto;\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "And then eventually get our acknowledgement. Now we just performed two write operations, and it required two round trips to the database.\n",
    "\n",
    "That's a round trip to the database for each write operation - but if we already knew all the writes we wanted to perform, why is our client sending them each one at a time? You probably see where this is going."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3.amazonaws.com/edu-static.mongodb.com/lessons/M220/notebook_assets/replica_set_update_many_ordered.png\" style=\"margin: 0 auto;\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "So what we can do instead is \"batch\" these inserts together, and then send them in bulk. The exact method of grouping documents together is implemented differently in each driver, because the data structures are different, but the general idea is the same: Package a bunch of writes into a batch, usually a list or array (but again, the implementation is different in each language), and then send that whole batch to MongoDB.\n",
    "\n",
    "This is the implementation of bulk writes in the Mongo shell, and you can copy this from the lecture notes if you want to try it out. But it will look different in your chosen programming language, so bear that in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3.amazonaws.com/edu-static.mongodb.com/lessons/M220/notebook_assets/replica_set_update_many_ordered_ack.png\" style=\"margin: 0 auto;\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "When a client sends a bulk write, it gets one acknowledgement back from the database for the whole batch.\n",
    "\n",
    "This is a benefit to our application's performance, because it limits the effect of latency on the overall speed of the operation. If it takes one second for each round trip, then sending one write at a time takes four seconds. But if we can send all four writes in one round trip, then sending four writes only takes one second.\n",
    "\n",
    "Now, the default behavior of a bulk write in Mongo is an ordered execution of these (point) writes (point). And in the ordered bulk write, any failure will stop execution of the rest of the batch. This benefits us in this case, because these (point) writes might have an effect on each other, like if two different update operations want to buy 4 sticks of butter, but there's only one stick left. In that situation, the first operation in the batch should get the last stick of butter, and the second operation should error out. That's why we need these (point) executed in order.\n",
    "\n",
    "The bulk write would throw an error on this (point) update statement, and then return an acknowledgement to the client before trying to purchase anymore (point) items. The acknowledgement (point) we get back will then tell us if something errored out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordered Bulk Write"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- The default setting for bulk writes in MongoDB\n",
    "- Executes writes sequentially\n",
    "- Will end execution after first write failure"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "By default, bulk writes in Mongo will be ordered. This means that even though we send all the writes at the same time, the replica (point) set will apply them in the order they were sent.\n",
    "\n",
    "Sending an ordered bulk write implies that each write (point to butter or eggs) in the batch depends on all the writes (point to the ones above it) that occurred before it. So if a write operation results in an error, all subsequent writes will not be executed, because Mongo assumes that those (point to ones below it) writes were expecting this (point) write to succeed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3.amazonaws.com/edu-static.mongodb.com/lessons/M220/notebook_assets/replica_set_update_many_unordered.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there's also a chance that the writes in our batch are not dependent on each other. In this case, we've just received a shipment of food to the warehouse, and we want to update the new food quantities in stock.\n",
    "\n",
    "Because all these (point) changes are additive, we don't need all of them to be executed in order, because they're not causally related. So I passed this \"ordered: false\" flag to the bulk write command, which will execute them in parallel. If some of them fail (for whatever reason), we can still continue on with the execution of other operations in the batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3.amazonaws.com/edu-static.mongodb.com/lessons/M220/notebook_assets/replica_set_update_many_unordered_ack.png\"/>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "And when we receive an acknowledgement back from the database, it will let us know if any operations failed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unordered Bulk Write"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- Has to be specified with the flag: { ordered: false }\n",
    "- Executes writes in parallel"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "If the writes in our batch don't have any causal relationship, then the client can send them over in an \"Unordered Bulk Write\". This will execute the write operations in parallel, so the writes are non-blocking. And as a result, a single failure won't prevent any of the other writes from succeeding. Now those writes might fail on their own, but their execution is not tied to the success of any other writes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Bulk writes allow database clients to send multiple writes\n",
    "# Can either be ordered or unordered\n",
    "\n",
    "One small thing to note: in a sharded collection, ordered bulk writes are expected to take longer because different write operations need to be routed to their designated shards. An ordered bulk write might reach the mongos in one batch, but then it has to be serialized across the different shards. Regardless of their designated shard, the write operation needs to be evaluated to see if we should continue or exit the execution of the remainder of the batch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
